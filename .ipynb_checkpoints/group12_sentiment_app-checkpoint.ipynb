{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import contractions\n",
    "import emoji\n",
    "import string \n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "# from transformers.modeling_bert import BertModel\n",
    "# from transformers import BertTokenizer, BertConfig\n",
    "# from transformers import BertForSequenceClassification\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras_preprocessing.text import Tokenizer\n",
    "\n",
    "from ipywidgets import widgets\n",
    "from ipylab import JupyterFrontEnd\n",
    "from IPython.display import Javascript, display\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WHAT is this? LOVED. COME ON! !@#! get rid of punctuations\n",
    "# Contraction words I've = I have\n",
    "# removal of stop words\n",
    "# removal of numbers\n",
    "\n",
    "# Remove emojis \n",
    "def remove_emojis(data):\n",
    "    emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  \n",
    "        u\"\\U0001F300-\\U0001F5FF\"  \n",
    "        u\"\\U0001F680-\\U0001F6FF\"  \n",
    "        u\"\\U0001F1E0-\\U0001F1FF\" \n",
    "        u\"\\U00002500-\\U00002BEF\"  \n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  \n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "    return re.sub(emoj, '', data)\n",
    "\n",
    "\n",
    "# change to lower_case\n",
    "def lower_case(review):\n",
    "    \n",
    "    return review.lower()\n",
    "\n",
    "# change contraction words such sa I'm = I am, shouldn't = should not\n",
    "def change_contractions(review):\n",
    "    \n",
    "    expanded_words = [contractions.fix(word) for word in review.split()]\n",
    "\n",
    "    expanded_review = ' '.join(expanded_words)\n",
    "    return expanded_review\n",
    "\n",
    "# Remove Punctuations\n",
    "def remove_punctuations(review):\n",
    "  \n",
    "  regex = re.compile('[^a-zA-Z0-9]')\n",
    "  #first parameter is the replacement, second parameter is your input string\n",
    "  new_review = regex.sub(' ', review)\n",
    "  return new_review\n",
    "\n",
    "# Remove numbers, we choose to remove numbers is because we find that for instance a review is descrbing about something \"the 2 girls in the book is so cute\"\n",
    "# it could lead to 2 star review. Because we realize that, those negative reviews user, will include numbers in their reviews.\n",
    "# Example: I give 2 star is because, the book is completely no link, also the words used in the book have a lot of grammatical error\n",
    "# As classification is supervised learning model, it is trained by using the corpus with respect to the (sentiment category). Thus, \n",
    "# if the number 2 appear even in a positive comment, it may in the end up in the negative comment depending on how strong the number 2 in the corpus is.\n",
    "# Thus, this will make the predictions go wrong. To reduce confusion for the model to learn, we decided to remove numbers\n",
    "def remove_numbers(review):\n",
    "    \n",
    "    mapping = str.maketrans('', '', string.digits)\n",
    "    new_review = review.translate(mapping)\n",
    "    \n",
    "    return new_review\n",
    "\n",
    "# Remove extra whitespaces\n",
    "def remove_extra_whitespace(reviews):\n",
    "    return \" \".join(reviews.split())\n",
    "\n",
    "# We dont want to remove words that are from the whitelist, the reason is because it can have better meaning in our sentences\n",
    "# The reason why i dont want remove one two three four etc is because, for instance if a user comment One start, this could mean it is a negative review straight away\n",
    "def remove_stopwords(text):\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    whitelist = [\"not\", \"no\", \"cannot\", \"do\", \"must\", \"should\", \"would\", \"could\"]\n",
    "    words = text.split() \n",
    "    clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1]\n",
    "    return \" \".join(clean_words)\n",
    "\n",
    "def get_wordnet_pos(text):\n",
    "    # Map POS tag to first character lemmatize() accepts\n",
    "    tags = nltk.pos_tag(text)\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    tags = [tag_dict.get(tag[1][0],  wordnet.NOUN) for tag in tags]\n",
    "    return tags\n",
    "\n",
    "def lemmaSentence(reviews):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemma_text = ''\n",
    "    tok_text = word_tokenize(reviews)\n",
    "    tags = get_wordnet_pos(tok_text)\n",
    "    for i in range(len(tok_text)):\n",
    "        lemma_text = lemma_text + ' ' + lemmatizer.lemmatize(tok_text[i], tags[i])\n",
    "    return lemma_text[1:] \n",
    "\n",
    "def clean_text(data):\n",
    "    data = lower_case(data)\n",
    "    data = change_contractions(data)\n",
    "    data = remove_emojis(data)\n",
    "    data = remove_punctuations(data)\n",
    "    data = remove_numbers(data)\n",
    "    data = remove_stopwords(data)\n",
    "    data = remove_extra_whitespace(data)\n",
    "    data = lemmaSentence(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_pred_result(model, original_text, cleaned_text, detected_polarity):\n",
    "    print(\"Original text sequence: \" + original_text)\n",
    "    print(\"Preprocessed text sequence: \" + cleaned_text)\n",
    "    print(\"Model: \" + model)\n",
    "    if original_text == \"\" or original_text is None:\n",
    "        detected_polarity = \"\"\n",
    "    print(\"Detected polarity: \" + detected_polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Trained Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./trained_models/Naive_Bayes_TFIDF_model.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[0;32m----> 2\u001b[0m     NB_trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "with open('./trained_models/Naive_Bayes_TFIDF_model.pkl', 'rb') as handle:\n",
    "    NB_trained_model = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers.modeling_bert'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [6], line 55\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[1;32m     54\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 55\u001b[0m BERT_trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./trained_models/BERT_model.bin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m config \u001b[38;5;241m=\u001b[39m BertConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     57\u001b[0m BERT_tokenizer \u001b[38;5;241m=\u001b[39m BertTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, do_lower_case\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/CZ-CE4045_NLP_Group_12-QxQI1UgB/lib/python3.9/site-packages/torch/serialization.py:789\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    787\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    788\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m--> 789\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights_only:\n\u001b[1;32m    791\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/CZ-CE4045_NLP_Group_12-QxQI1UgB/lib/python3.9/site-packages/torch/serialization.py:1131\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1129\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1130\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1131\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1133\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/CZ-CE4045_NLP_Group_12-QxQI1UgB/lib/python3.9/site-packages/torch/serialization.py:1124\u001b[0m, in \u001b[0;36m_load.<locals>.UnpicklerWrapper.find_class\u001b[0;34m(self, mod_name, name)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1123\u001b[0m mod_name \u001b[38;5;241m=\u001b[39m load_module_mapping\u001b[38;5;241m.\u001b[39mget(mod_name, mod_name)\n\u001b[0;32m-> 1124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers.modeling_bert'"
     ]
    }
   ],
   "source": [
    "class SentimentData(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.text = dataframe.text\n",
    "        self.targets = self.data.polarity\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.text[index])\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
    "        }\n",
    "\n",
    "class BertClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BertClass, self).__init__()\n",
    "        self.l1 = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
    "        self.dropout = torch.nn.Dropout(0.2)\n",
    "        self.classifier = torch.nn.Linear(768, 2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        hidden_state = output_1[0]\n",
    "        pooler = hidden_state[:, 0]\n",
    "        pooler = self.pre_classifier(pooler)\n",
    "        pooler = torch.nn.ReLU()(pooler)\n",
    "        pooler = self.dropout(pooler)\n",
    "        output = self.classifier(pooler)\n",
    "        return output\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "BERT_trained_model = torch.load('./trained_models/BERT_model.bin', map_location=device)\n",
    "config = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
    "BERT_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', truncation=True, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BERT_predict(input_loader):\n",
    "  preds = []\n",
    "  for _, data in tqdm(enumerate(input_loader, 0)):\n",
    "    ids = data['ids'].to(device, dtype = torch.long)\n",
    "    mask = data['mask'].to(device, dtype = torch.long)\n",
    "    token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "    targets = data['targets'].to(device, dtype = torch.long)\n",
    "    pred = BERT_trained_model(ids, mask, token_type_ids)\n",
    "    big_val, big_idx = torch.max(pred.data, dim=1)\n",
    "    if big_idx == 1:\n",
    "      preds.append('Postive')\n",
    "    else:\n",
    "      preds.append('Negative')\n",
    "  return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-05 13:51:58.644710: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "LSTM_trained_model = load_model('./trained_models/LSTM_model.h5')\n",
    "\n",
    "with open('./trained_models/LSTM_tokenizer.pickle', 'rb') as handle:\n",
    "    LSTM_tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polarity Detection on Text Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb9fa85b87d84f768eb22bc97b258839",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='Text Sequence')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01c7d2d776e1405daa950a7fc60cb536",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# app = JupyterFrontEnd()\n",
    "\n",
    "# app.commands.list_commands()\n",
    "\n",
    "original_text_label = widgets.Label(\"Text Sequence\")\n",
    "original_text_input = widgets.Text()\n",
    "display(original_text_label)\n",
    "display(original_text_input)\n",
    "\n",
    "# submit_button = widgets.Button(description=\"Detect polarity!\")\n",
    "# display(submit_button)\n",
    "\n",
    "# def run_all(ev):\n",
    "#     app.commands.execute('notebook:run-all-below')\n",
    "\n",
    "# submit_button.on_click(run_all)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_text = original_text_input.value\n",
    "cleaned_text = clean_text(original_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Polarity Detection (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google BERT Polarity Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BERT_tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [19], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m: [cleaned_text], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpolarity\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m1\u001b[39m]}\n\u001b[1;32m      3\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m BERT_input \u001b[38;5;241m=\u001b[39m DataLoader(SentimentData(df, \u001b[43mBERT_tokenizer\u001b[49m, \u001b[38;5;241m256\u001b[39m))\n\u001b[1;32m      5\u001b[0m BERT_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(BERT_predict(BERT_input)[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m      6\u001b[0m print_pred_result(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Google BERT]\u001b[39m\u001b[38;5;124m\"\u001b[39m, original_text, cleaned_text, BERT_pred)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BERT_tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# Google BERT\n",
    "input = {'text': [cleaned_text], 'polarity': [1]}\n",
    "df = pd.DataFrame(input)\n",
    "BERT_input = DataLoader(SentimentData(df, BERT_tokenizer, 256))\n",
    "BERT_pred = str(BERT_predict(BERT_input)[0])\n",
    "print_pred_result(\"[Google BERT]\", original_text, cleaned_text, BERT_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Polarity Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 89ms/step\n",
      "Original text sequence: This book good baby tbh...\n",
      "Preprocessed text sequence: book good baby honest\n",
      "Model: [LSTM]\n",
      "Detected polarity: Positive\n"
     ]
    }
   ],
   "source": [
    "# LSTM\n",
    "token_text = pad_sequences(LSTM_tokenizer.texts_to_sequences([cleaned_text]), maxlen=1000)\n",
    "polarity = LSTM_trained_model.predict(token_text)[0][0]\n",
    "if polarity < 0.5:\n",
    "    LSTM_pred = \"Negative\"\n",
    "else:\n",
    "    LSTM_pred = \"Positive\"\n",
    "\n",
    "print_pred_result(\"[LSTM]\", original_text, cleaned_text, LSTM_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CZ-CE4045_NLP_Group_12",
   "language": "python",
   "name": "cz-ce4045_nlp_group_12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "65a8742737a1feb0368b249bfcd2173157ebb656fda0f57e6aa8d90bd5a9a205"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
