{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-05 16:01:34.936083: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import joblib\n",
    "\n",
    "import contractions\n",
    "import string \n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "# from transformers.modeling_bert import BertModel\n",
    "# from transformers import BertTokenizer, BertConfig\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "from ipywidgets import widgets\n",
    "from ipylab import JupyterFrontEnd\n",
    "from IPython.display import Javascript, display\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WHAT is this? LOVED. COME ON! !@#! get rid of punctuations\n",
    "# Contraction words I've = I have\n",
    "# removal of stop words\n",
    "# removal of numbers\n",
    "\n",
    "# Remove emojis \n",
    "def remove_emojis(data):\n",
    "    emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  \n",
    "        u\"\\U0001F300-\\U0001F5FF\"  \n",
    "        u\"\\U0001F680-\\U0001F6FF\"  \n",
    "        u\"\\U0001F1E0-\\U0001F1FF\" \n",
    "        u\"\\U00002500-\\U00002BEF\"  \n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  \n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "    return re.sub(emoj, '', data)\n",
    "\n",
    "\n",
    "# change to lower_case\n",
    "def lower_case(review):\n",
    "    \n",
    "    return review.lower()\n",
    "\n",
    "# change contraction words such sa I'm = I am, shouldn't = should not\n",
    "def change_contractions(review):\n",
    "    \n",
    "    expanded_words = [contractions.fix(word) for word in review.split()]\n",
    "\n",
    "    expanded_review = ' '.join(expanded_words)\n",
    "    return expanded_review\n",
    "\n",
    "# Remove Punctuations\n",
    "def remove_punctuations(review):\n",
    "  \n",
    "  regex = re.compile('[^a-zA-Z0-9]')\n",
    "  #first parameter is the replacement, second parameter is your input string\n",
    "  new_review = regex.sub(' ', review)\n",
    "  return new_review\n",
    "\n",
    "# Remove numbers, we choose to remove numbers is because we find that for instance a review is descrbing about something \"the 2 girls in the book is so cute\"\n",
    "# it could lead to 2 star review. Because we realize that, those negative reviews user, will include numbers in their reviews.\n",
    "# Example: I give 2 star is because, the book is completely no link, also the words used in the book have a lot of grammatical error\n",
    "# As classification is supervised learning model, it is trained by using the corpus with respect to the (sentiment category). Thus, \n",
    "# if the number 2 appear even in a positive comment, it may in the end up in the negative comment depending on how strong the number 2 in the corpus is.\n",
    "# Thus, this will make the predictions go wrong. To reduce confusion for the model to learn, we decided to remove numbers\n",
    "def remove_numbers(review):\n",
    "    \n",
    "    mapping = str.maketrans('', '', string.digits)\n",
    "    new_review = review.translate(mapping)\n",
    "    \n",
    "    return new_review\n",
    "\n",
    "# Remove extra whitespaces\n",
    "def remove_extra_whitespace(reviews):\n",
    "    return \" \".join(reviews.split())\n",
    "\n",
    "# We dont want to remove words that are from the whitelist, the reason is because it can have better meaning in our sentences\n",
    "# The reason why i dont want remove one two three four etc is because, for instance if a user comment One start, this could mean it is a negative review straight away\n",
    "def remove_stopwords(text):\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    whitelist = [\"not\", \"no\", \"cannot\", \"do\", \"must\", \"should\", \"would\", \"could\"]\n",
    "    words = text.split() \n",
    "    clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1]\n",
    "    return \" \".join(clean_words)\n",
    "\n",
    "def get_wordnet_pos(text):\n",
    "    # Map POS tag to first character lemmatize() accepts\n",
    "    tags = nltk.pos_tag(text)\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    tags = [tag_dict.get(tag[1][0],  wordnet.NOUN) for tag in tags]\n",
    "    return tags\n",
    "\n",
    "def lemmaSentence(reviews):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemma_text = ''\n",
    "    tok_text = word_tokenize(reviews)\n",
    "    tags = get_wordnet_pos(tok_text)\n",
    "    for i in range(len(tok_text)):\n",
    "        lemma_text = lemma_text + ' ' + lemmatizer.lemmatize(tok_text[i], tags[i])\n",
    "    return lemma_text[1:] \n",
    "\n",
    "def clean_text(data, single_input=True):\n",
    "    if single_input:\n",
    "        data = lower_case(data)\n",
    "        data = change_contractions(data)\n",
    "        data = remove_emojis(data)\n",
    "        data = remove_punctuations(data)\n",
    "        data = remove_numbers(data)\n",
    "        data = remove_stopwords(data)\n",
    "        data = remove_extra_whitespace(data)\n",
    "        data = lemmaSentence(data)\n",
    "    else:\n",
    "        data['concat_review'] = data['concat_review'].apply(lower_case)\n",
    "        data['concat_review'] = data['concat_review'].apply(change_contractions)\n",
    "        data['concat_review'] = data['concat_review'].apply(remove_emojis)\n",
    "        data['concat_review'] = data['concat_review'].apply(remove_punctuations)\n",
    "        data['concat_review'] = data['concat_review'].apply(remove_numbers)\n",
    "        data['concat_review'] = data['concat_review'].apply(remove_stopwords)\n",
    "        data['concat_review'] = data['concat_review'].apply(remove_extra_whitespace)\n",
    "        data['concat_review'] = data['concat_review'].apply(lemmaSentence)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_pred_result(model, original_text, cleaned_text, detected_polarity):\n",
    "    print(\"Original text sequence: \" + original_text)\n",
    "    print(\"Preprocessed text sequence: \" + cleaned_text)\n",
    "    print(\"Model: \" + model)\n",
    "    if original_text == \"\" or original_text is None:\n",
    "        detected_polarity = \"\"\n",
    "    elif detected_polarity > 0.5:\n",
    "        detected_polarity = \"Positive\"\n",
    "    else:\n",
    "        detected_polarity = \"Negative\"\n",
    "    print(\"Detected polarity: \" + detected_polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Trained Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting TF-IDF with Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('./train_test_dataset/train_df_imbalanced.csv')\n",
    "clean_train_data_df = clean_text(train_data, single_input=False)\n",
    "differences = clean_train_data_df[\"polarity\"].value_counts()[1]-clean_train_data_df[\"polarity\"].value_counts()[-1]\n",
    "train_balanced_df = clean_train_data_df.drop(clean_train_data_df[clean_train_data_df[\"polarity\"] == 1].sample(differences,random_state=42).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<15233x10000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 547653 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = np.array(train_balanced_df[\"concat_review\"].values)\n",
    "tfidf_vect = TfidfVectorizer(min_df=5, max_features=10000, ngram_range=(1,2), lowercase=False, tokenizer=word_tokenize)\n",
    "tfidf_vect.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bryson/.local/share/virtualenvs/CZ-CE4045_NLP_Group_12-QxQI1UgB/lib/python3.9/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator MultinomialNB from version 1.1.0 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "with open('./trained_models/Naive_Bayes_TFIDF_model.pkl', 'rb') as handle:\n",
    "    NB_trained_model = joblib.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVC (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bryson/.local/share/virtualenvs/CZ-CE4045_NLP_Group_12-QxQI1UgB/lib/python3.9/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator LinearSVC from version 1.1.0 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "with open('./trained_models/Linear_SVC_TFIDF_model.pkl', 'rb') as handle:\n",
    "    LinearSVC_trained_model = joblib.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bryson/.local/share/virtualenvs/CZ-CE4045_NLP_Group_12-QxQI1UgB/lib/python3.9/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.1.0 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "with open('./trained_models/Logistic_Regression_TFIDF_model.pkl', 'rb') as handle:\n",
    "    LogisticRegression_trained_model = joblib.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-05 16:02:34.789617: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "LSTM_trained_model = load_model('./trained_models/LSTM_model.h5')\n",
    "\n",
    "with open('./trained_models/LSTM_tokenizer.pickle', 'rb') as handle:\n",
    "    LSTM_tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SentimentData(Dataset):\n",
    "#     def __init__(self, dataframe, tokenizer, max_len):\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.data = dataframe\n",
    "#         self.text = dataframe.text\n",
    "#         self.targets = self.data.polarity\n",
    "#         self.max_len = max_len\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.text)\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         text = str(self.text[index])\n",
    "#         text = \" \".join(text.split())\n",
    "\n",
    "#         inputs = self.tokenizer.encode_plus(\n",
    "#             text,\n",
    "#             None,\n",
    "#             add_special_tokens=True,\n",
    "#             max_length=self.max_len,\n",
    "#             pad_to_max_length=True,\n",
    "#             return_token_type_ids=True\n",
    "#         )\n",
    "#         ids = inputs['input_ids']\n",
    "#         mask = inputs['attention_mask']\n",
    "#         token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "\n",
    "#         return {\n",
    "#             'ids': torch.tensor(ids, dtype=torch.long),\n",
    "#             'mask': torch.tensor(mask, dtype=torch.long),\n",
    "#             'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "#             'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
    "#         }\n",
    "\n",
    "# class BertClass(torch.nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(BertClass, self).__init__()\n",
    "#         self.l1 = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "#         self.pre_classifier = torch.nn.Linear(768, 768)\n",
    "#         self.dropout = torch.nn.Dropout(0.2)\n",
    "#         self.classifier = torch.nn.Linear(768, 2)\n",
    "\n",
    "#     def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "#         output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "#         hidden_state = output_1[0]\n",
    "#         pooler = hidden_state[:, 0]\n",
    "#         pooler = self.pre_classifier(pooler)\n",
    "#         pooler = torch.nn.ReLU()(pooler)\n",
    "#         pooler = self.dropout(pooler)\n",
    "#         output = self.classifier(pooler)\n",
    "#         return output\n",
    "\n",
    "# def BERT_predict(input_loader):\n",
    "#   preds = []\n",
    "#   for _, data in tqdm(enumerate(input_loader, 0)):\n",
    "#     ids = data['ids'].to(device, dtype = torch.long)\n",
    "#     mask = data['mask'].to(device, dtype = torch.long)\n",
    "#     token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "#     targets = data['targets'].to(device, dtype = torch.long)\n",
    "#     pred = BERT_trained_model(ids, mask, token_type_ids)\n",
    "#     big_val, big_idx = torch.max(pred.data, dim=1)\n",
    "#     if big_idx == 1:\n",
    "#       preds.append(1)\n",
    "#     else:\n",
    "#       preds.append(0)\n",
    "#   return preds\n",
    "  \n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# BERT_trained_model = torch.load('./trained_models/BERT_model.bin', map_location=device)\n",
    "# config = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
    "# BERT_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', truncation=True, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polarity Detection on Text Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc80b5db3a354a9487dd6999b8ff3251",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='Text Sequence')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0de288b86a014fcb9e97b33b61d6ed55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# app = JupyterFrontEnd()\n",
    "\n",
    "# app.commands.list_commands()\n",
    "\n",
    "original_text_label = widgets.Label(\"Text Sequence\")\n",
    "original_text_input = widgets.Text()\n",
    "display(original_text_label)\n",
    "display(original_text_input)\n",
    "\n",
    "# submit_button = widgets.Button(description=\"Detect polarity!\")\n",
    "# display(submit_button)\n",
    "\n",
    "# def run_all(ev):\n",
    "#     app.commands.execute('notebook:run-all-below')\n",
    "\n",
    "# submit_button.on_click(run_all)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_text = original_text_input.value\n",
    "cleaned_text = clean_text(original_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Polarity Detection (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text sequence: This book is really lovely and easy to read\n",
      "Preprocessed text sequence: book really lovely easy read\n",
      "Model: [Naive Bayes, TF-IDF]\n",
      "Detected polarity: Negative\n"
     ]
    }
   ],
   "source": [
    "input = np.array([cleaned_text])\n",
    "tfidf_input = tfidf_vect.transform(input)\n",
    "\n",
    "NB_pred = NB_trained_model.predict(tfidf_input)[0]\n",
    "print_pred_result(\"[Naive Bayes, TF-IDF]\", original_text, cleaned_text, NB_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVC Polarity Detection (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text sequence: This book is really lovely and easy to read\n",
      "Preprocessed text sequence: book really lovely easy read\n",
      "Model: [Linear SVC, TF-IDF]\n",
      "Detected polarity: Positive\n"
     ]
    }
   ],
   "source": [
    "input = np.array([cleaned_text])\n",
    "tfidf_input = tfidf_vect.transform(input)\n",
    "\n",
    "LinearSVC_pred = LinearSVC_trained_model.predict(tfidf_input)[0]\n",
    "print_pred_result(\"[Linear SVC, TF-IDF]\", original_text, cleaned_text, LinearSVC_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Logistic Polarity Detection (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text sequence: This book is really lovely and easy to read\n",
      "Preprocessed text sequence: book really lovely easy read\n",
      "Model: [Logistic Regression, TF-IDF]\n",
      "Detected polarity: Negative\n"
     ]
    }
   ],
   "source": [
    "input = np.array([cleaned_text])\n",
    "tfidf_input = tfidf_vect.transform(input)\n",
    "\n",
    "LinearLogistic_pred = LogisticRegression_trained_model.predict(tfidf_input)[0]\n",
    "print_pred_result(\"[Logistic Regression, TF-IDF]\", original_text, cleaned_text, LinearLogistic_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Polarity Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 357ms/step\n",
      "Original text sequence: This book is really lovely and easy to read\n",
      "Preprocessed text sequence: book really lovely easy read\n",
      "Model: [LSTM]\n",
      "Detected polarity: Positive\n"
     ]
    }
   ],
   "source": [
    "# LSTM\n",
    "token_text = pad_sequences(LSTM_tokenizer.texts_to_sequences([cleaned_text]), maxlen=1000)\n",
    "\n",
    "LSTM_pred = LSTM_trained_model.predict(token_text)[0][0]\n",
    "print_pred_result(\"[LSTM]\", original_text, cleaned_text, LSTM_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google BERT Polarity Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Google BERT\n",
    "# input = {'text': [cleaned_text], 'polarity': [1]}\n",
    "# df = pd.DataFrame(input)\n",
    "# BERT_input = DataLoader(SentimentData(df, BERT_tokenizer, 256))\n",
    "# BERT_pred = BERT_predict(BERT_input)[0]\n",
    "\n",
    "# print_pred_result(\"[Google BERT]\", original_text, cleaned_text, BERT_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CZ-CE4045_NLP_Group_12",
   "language": "python",
   "name": "cz-ce4045_nlp_group_12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "65a8742737a1feb0368b249bfcd2173157ebb656fda0f57e6aa8d90bd5a9a205"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
