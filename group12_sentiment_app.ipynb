{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import contractions\n",
    "import emoji\n",
    "import string \n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers.modeling_bert import BertModel\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "from keras.models import load_model\n",
    "import pickle\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras_preprocessing.text import Tokenizer\n",
    "\n",
    "from ipywidgets import widgets\n",
    "from ipylab import JupyterFrontEnd\n",
    "from IPython.display import Javascript, display\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WHAT is this? LOVED. COME ON! !@#! get rid of punctuations\n",
    "# Contraction words I've = I have\n",
    "# removal of stop words\n",
    "# removal of numbers\n",
    "\n",
    "# Remove emojis \n",
    "def remove_emojis(data):\n",
    "    emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  \n",
    "        u\"\\U0001F300-\\U0001F5FF\"  \n",
    "        u\"\\U0001F680-\\U0001F6FF\"  \n",
    "        u\"\\U0001F1E0-\\U0001F1FF\" \n",
    "        u\"\\U00002500-\\U00002BEF\"  \n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  \n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "    return re.sub(emoj, '', data)\n",
    "\n",
    "\n",
    "# change to lower_case\n",
    "def lower_case(review):\n",
    "    \n",
    "    return review.lower()\n",
    "\n",
    "# change contraction words such sa I'm = I am, shouldn't = should not\n",
    "def change_contractions(review):\n",
    "    \n",
    "    expanded_words = [contractions.fix(word) for word in review.split()]\n",
    "\n",
    "    expanded_review = ' '.join(expanded_words)\n",
    "    return expanded_review\n",
    "\n",
    "# Remove Punctuations\n",
    "def remove_punctuations(review):\n",
    "  \n",
    "  regex = re.compile('[^a-zA-Z0-9]')\n",
    "  #first parameter is the replacement, second parameter is your input string\n",
    "  new_review = regex.sub(' ', review)\n",
    "  return new_review\n",
    "\n",
    "# Remove numbers, we choose to remove numbers is because we find that for instance a review is descrbing about something \"the 2 girls in the book is so cute\"\n",
    "# it could lead to 2 star review. Because we realize that, those negative reviews user, will include numbers in their reviews.\n",
    "# Example: I give 2 star is because, the book is completely no link, also the words used in the book have a lot of grammatical error\n",
    "# As classification is supervised learning model, it is trained by using the corpus with respect to the (sentiment category). Thus, \n",
    "# if the number 2 appear even in a positive comment, it may in the end up in the negative comment depending on how strong the number 2 in the corpus is.\n",
    "# Thus, this will make the predictions go wrong. To reduce confusion for the model to learn, we decided to remove numbers\n",
    "def remove_numbers(review):\n",
    "    \n",
    "    mapping = str.maketrans('', '', string.digits)\n",
    "    new_review = review.translate(mapping)\n",
    "    \n",
    "    return new_review\n",
    "\n",
    "# Remove extra whitespaces\n",
    "def remove_extra_whitespace(reviews):\n",
    "    return \" \".join(reviews.split())\n",
    "\n",
    "# We dont want to remove words that are from the whitelist, the reason is because it can have better meaning in our sentences\n",
    "# The reason why i dont want remove one two three four etc is because, for instance if a user comment One start, this could mean it is a negative review straight away\n",
    "def remove_stopwords(text):\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    whitelist = [\"not\", \"no\", \"cannot\", \"do\", \"must\", \"should\", \"would\", \"could\"]\n",
    "    words = text.split() \n",
    "    clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1]\n",
    "    return \" \".join(clean_words)\n",
    "\n",
    "def get_wordnet_pos(text):\n",
    "    # Map POS tag to first character lemmatize() accepts\n",
    "    tags = nltk.pos_tag(text)\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    tags = [tag_dict.get(tag[1][0],  wordnet.NOUN) for tag in tags]\n",
    "    return tags\n",
    "\n",
    "def lemmaSentence(reviews):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemma_text = ''\n",
    "    tok_text = word_tokenize(reviews)\n",
    "    tags = get_wordnet_pos(tok_text)\n",
    "    for i in range(len(tok_text)):\n",
    "        lemma_text = lemma_text + ' ' + lemmatizer.lemmatize(tok_text[i], tags[i])\n",
    "    return lemma_text[1:] \n",
    "\n",
    "def clean_text(data):\n",
    "    data = lower_case(data)\n",
    "    data = change_contractions(data)\n",
    "    data = remove_emojis(data)\n",
    "    data = remove_punctuations(data)\n",
    "    data = remove_numbers(data)\n",
    "    data = remove_stopwords(data)\n",
    "    data = remove_extra_whitespace(data)\n",
    "    data = lemmaSentence(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_pred_result(model, original_text, cleaned_text, detected_polarity):\n",
    "    print(\"Original text sequence: \" + original_text)\n",
    "    print(\"Input text sequence: \" + cleaned_text)\n",
    "    if original_text == \"\" or original_text is None:\n",
    "        detected_polarity = \"\"\n",
    "    print(model + \" Polarity detected: \" + detected_polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Trained Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentData(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.text = dataframe.text\n",
    "        self.targets = self.data.polarity\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.text[index])\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
    "        }\n",
    "\n",
    "class BertClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BertClass, self).__init__()\n",
    "        self.l1 = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
    "        self.dropout = torch.nn.Dropout(0.2)\n",
    "        self.classifier = torch.nn.Linear(768, 2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        hidden_state = output_1[0]\n",
    "        pooler = hidden_state[:, 0]\n",
    "        pooler = self.pre_classifier(pooler)\n",
    "        pooler = torch.nn.ReLU()(pooler)\n",
    "        pooler = self.dropout(pooler)\n",
    "        output = self.classifier(pooler)\n",
    "        return output\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "BERT_trained_model = torch.load('./trained_models/BERT_model.bin', map_location=device)\n",
    "config = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
    "BERT_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', truncation=True, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BERT_predict(input_loader):\n",
    "  preds = []\n",
    "  for _, data in tqdm(enumerate(input_loader, 0)):\n",
    "    ids = data['ids'].to(device, dtype = torch.long)\n",
    "    mask = data['mask'].to(device, dtype = torch.long)\n",
    "    token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "    targets = data['targets'].to(device, dtype = torch.long)\n",
    "    pred = BERT_trained_model(ids, mask, token_type_ids)\n",
    "    big_val, big_idx = torch.max(pred.data, dim=1)\n",
    "    if big_idx == 1:\n",
    "      preds.append('Postive')\n",
    "    else:\n",
    "      preds.append('Negative')\n",
    "  return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_trained_model = load_model('./trained_models/LSTM_model.h5')\n",
    "\n",
    "with open('./trained_models/LSTM_tokenizer.pickle', 'rb') as handle:\n",
    "    LSTM_tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polarity Detection on Text Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f15c774d5b84404b89f708fcbd2b5292",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='Text Sequence')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7789c6380114b6890205e93846f64a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "app = JupyterFrontEnd()\n",
    "\n",
    "app.commands.list_commands()\n",
    "\n",
    "original_text_label = widgets.Label(\"Text Sequence\")\n",
    "original_text_input = widgets.Text()\n",
    "display(original_text_label)\n",
    "display(original_text_input)\n",
    "\n",
    "# submit_button = widgets.Button(description=\"Detect polarity!\")\n",
    "# display(submit_button)\n",
    "\n",
    "# def run_all(ev):\n",
    "#     app.commands.execute('notebook:run-all-below')\n",
    "\n",
    "# submit_button.on_click(run_all)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_text = original_text_input.value\n",
    "cleaned_text = clean_text(original_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "1it [00:00,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text sequence: This book is a good buy! Worth the money~~\n",
      "Input text sequence: book good buy worth money\n",
      "[Google BERT] Polarity detected: Postive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Google BERT\n",
    "input = {'text': [cleaned_text], 'polarity': [1]}\n",
    "df = pd.DataFrame(input)\n",
    "BERT_input = DataLoader(SentimentData(df, BERT_tokenizer, 256))\n",
    "BERT_pred = str(BERT_predict(BERT_input)[0])\n",
    "print_pred_result(\"[Google BERT]\", original_text, cleaned_text, BERT_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 121ms/step\n",
      "Original text sequence: This book is a good buy! Worth the money~~\n",
      "Input text sequence: book good buy worth money\n",
      "[LSTM] Polarity detected: Negative\n"
     ]
    }
   ],
   "source": [
    "# LSTM\n",
    "token_text = pad_sequences(LSTM_tokenizer.texts_to_sequences([cleaned_text]), maxlen=1000)\n",
    "polarity = LSTM_trained_model.predict(token_text)[0][0]\n",
    "if polarity < 0.5:\n",
    "    LSTM_pred = \"Negative\"\n",
    "else:\n",
    "    LSTM_pred = \"Positive\"\n",
    "\n",
    "print_pred_result(\"[LSTM]\", original_text, cleaned_text, LSTM_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit ('anaconda3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "65a8742737a1feb0368b249bfcd2173157ebb656fda0f57e6aa8d90bd5a9a205"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
